{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e67785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset\n",
      "Plain prompt\n",
      "Fine-tuned\n",
      "T5-small\n",
      "SDXL\n",
      "Gene\n"
     ]
    }
   ],
   "source": [
    "import fitz # PyMuPDF\n",
    "\n",
    "pdf_path = r\"C:\\Users\\shrinath\\OneDrive\\Desktop\\texttoimage.pdf\"\n",
    "def extract_text_pymupdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = []\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc.load_page(page_num)\n",
    "        text.append(page.get_text())\n",
    "    doc.close()\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "pdf_text = extract_text_pymupdf(pdf_path)\n",
    "print(pdf_text[2][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4ee044d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Dataset Plain prompt Fine-tuned T5-small SDXL Generated\\xa0 Images '\n",
      " 'Concatenate Evaluation Plain prompt Structured information Fig. 2. An '\n",
      " 'overview of the StructuredPrompter framework. language model during '\n",
      " 'empirical evaluation. In total, the validation set contains 100 prompts, '\n",
      " 'each paired with manually extracted structured information. 3.2. Augmented '\n",
      " 'Prompts Given a plain natural language prompt X as input and the '\n",
      " 'GPT-4o–extracted structured information Y as ground truth, we fine-tune the '\n",
      " 'language model f(·) to predict ˆY . The finetuning process is optimized with '\n",
      " 'cross-entropy loss, as shown in the following equations: ˆY = f(X), (1) LCE '\n",
      " '= − N X i=1 Yi log ˆYi, (2) where N is the number of tokens in the '\n",
      " 'structured output, Yi is the ground-truth token distribution at position i, '\n",
      " 'and ˆYi is the predicted probability distribution over the vocabulary at '\n",
      " 'position i. The training process is evaluated on the validation set every '\n",
      " 'five epochs. If performance does not improve within the subsequent five '\n",
      " 'epochs, we consider the training to have converged. In this work, we employ '\n",
      " 'T5-small as the language model for structured information extraction. On the '\n",
      " 'validation set, it achieved scores above 0.98 in both BLEU and ROUGE, '\n",
      " 'confirming its reliability. Its compact model size further demonstrates the '\n",
      " 'feasibility of lightweight tuple extraction. Detailed statistics are '\n",
      " 'reported in Table 1. The extracted structured information is then '\n",
      " 'concatenated with plain natural language prompts to form augmented prompts, '\n",
      " 'which are provided as inputs to Stable Diffusion XL for image generation. '\n",
      " 'This process requires no modifications to the diffusion model and adds '\n",
      " 'minimal overhead, making it portable across text-to-image models. 3.3. '\n",
      " 'Evaluation Spatial Relationships. We employ Qwen2.5-VL-3B-Instruct [30] as a '\n",
      " 'judge to verify whether the generated images correctly convey the '\n",
      " 'information specified in the plain prompts. For instance, given the prompt '\n",
      " '“B is on the right of A” and a generated image, we query '\n",
      " 'Qwen2.5-VL-3B-Instruct with “Is B on the right of A? Please answer Yes or '\n",
      " 'No.” A correctly generated image should support a Yes response. In addition '\n",
      " 'to spatial relationships, we also evaluate color and shape alignment. The '\n",
      " 'results are in the Table 2. Image Quality. We further assess the overall '\n",
      " 'visual quality of the generated images using the Inception Score [31]. 4. '\n",
      " 'EXPERIMENTS 4.1. Implementation Details We conduct experiments using three '\n",
      " 'random seeds (40, 41, and 42). The training batch size is 4 with a gradient '\n",
      " 'accumulation step of 8. The learning rate is set to 1e-4, optimized using '\n",
      " 'Adam with 10 warm-up steps and a weight decay of 0.01. All experiments are '\n",
      " 'implemented in PyTorch 2.5.1 and Transformers 4.50, and executed on an '\n",
      " 'NVIDIA A100 GPU. Additional details are available in the GitHub repository '\n",
      " 'of this work 1. 4.2. Impact Of Parameter Scale As shown in Table 1, we adopt '\n",
      " 'fine-tuned T5-small as the backbone model for structured information '\n",
      " 'extraction. Using 500 samples generated with GPT-4o, it achieves a BLEU '\n",
      " 'score of 0.98 and a ROUGE score of 0.99. These results demonstrate that '\n",
      " 'T5-small is a lightweight yet effective extractor of structured information. '\n",
      " 'In contrast, Llama-3.2-1B 1The code and data will be publicly available upon '\n",
      " 'publication at https://github.com/Sander445/StructuredPrompter.')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pprint\n",
    "\n",
    "def clean_academic_pdf_text(text):\n",
    "    # 1. Remove hyphenation across line breaks (e.g., \"val-\\nidation\" -> \"validation\")\n",
    "    text = re.sub(r'-\\n', '', text)\n",
    "    \n",
    "    # 2. Replace newlines within paragraphs with spaces\n",
    "    text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n",
    "    \n",
    "    # 3. Collapse multiple newlines into paragraph breaks\n",
    "    text = re.sub(r'\\n+', '\\n\\n', text)\n",
    "    \n",
    "    # 4. Remove isolated page numbers (if any)\n",
    "    text = re.sub(r'^\\s*\\d+\\s*$', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 5. Optional: remove figure/table captions\n",
    "    text = re.sub(r'Fig\\.\\s*\\d+.*?(?=\\n\\n)', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # 6. Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "text = clean_academic_pdf_text(pdf_text[2])\n",
    "pprint.pprint(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8f1c92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
